D:\involuntary\works\classes\ANN\HW1\codes\loss.py:39: RuntimeWarning: divide by zero encountered in log
  ln_t_sub_h = np.where(target == 0, 1e-9, np.log(target / input_softmax))
14:42:04.552 Testing @ 0 epoch...
14:42:06.764 Testing @ 1 epoch...
14:42:08.711 Testing @ 2 epoch...
14:42:10.626 Testing @ 3 epoch...
14:42:12.803 Testing @ 4 epoch...
14:42:15.305 Testing @ 5 epoch...
14:42:21.164 Testing @ 6 epoch...
14:42:28.345 Testing @ 7 epoch...
Traceback (most recent call last):
  File "D:\involuntary\works\classes\ANN\HW1\codes\run_mlp.py", line 115, in <module>
    processor(modelx, lossx, tr_data, te_data, tr_label, te_label, train_config)
  File "D:\involuntary\works\classes\ANN\HW1\codes\run_mlp.py", line 59, in processor
    iteration = train_net(mdl, los, config, train_data, train_label, config['batch_size'], config['disp_freq'])
  File "D:\involuntary\works\classes\ANN\HW1\codes\solve_net.py", line 33, in train_net
    model.backward(grad)
  File "D:\involuntary\works\classes\ANN\HW1\codes\network.py", line 21, in backward
    grad_input = self.layer_list[i].backward(grad_input)
  File "D:\involuntary\works\classes\ANN\HW1\codes\layers.py", line 138, in backward
    return ret.copy()
KeyboardInterrupt
