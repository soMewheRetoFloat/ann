Training:   0%|                                                                                                           | 0/100 [00:00<?, ?it/s]D:\involuntary\works\classes\ANN\HW1\codes\loss.py:39: RuntimeWarning: divide by zero encountered in log
  ln_t_sub_h = np.where(target == 0, 1e-9, np.log(target / input_softmax))
20:40:14.137      Training iter 50, batch loss 0.0223, batch acc 0.2634
20:40:14.377      Training iter 100, batch loss 0.0153, batch acc 0.6650
20:40:14.572      Training iter 150, batch loss 0.0082, batch acc 0.7952
20:40:14.679      Training iter 200, batch loss 0.0061, batch acc 0.8384
20:40:14.858      Training iter 250, batch loss 0.0051, batch acc 0.8666
20:40:15.049      Training iter 300, batch loss 0.0045, batch acc 0.8750
20:40:15.268      Training iter 350, batch loss 0.0043, batch acc 0.8754
20:40:15.500      Training iter 400, batch loss 0.0037, batch acc 0.8948
20:40:15.705      Training iter 450, batch loss 0.0036, batch acc 0.8988
20:40:15.932      Training iter 500, batch loss 0.0035, batch acc 0.8994
20:40:16.167      Training iter 550, batch loss 0.0036, batch acc 0.8942
20:40:16.407      Training iter 600, batch loss 0.0034, batch acc 0.9046
20:40:16.408 Testing @ 0 epoch...
20:40:16.553      Testing, total mean loss 0.00321, total acc 0.90720
Training:   3%|██▉                                                                                                | 3/100 [00:08<04:24,  2.73s/it]
20:40:16.815      Training iter 50, batch loss 0.0032, batch acc 0.9088
20:40:17.053      Training iter 100, batch loss 0.0032, batch acc 0.9032
20:40:17.271      Training iter 150, batch loss 0.0032, batch acc 0.9130
20:40:17.500      Training iter 200, batch loss 0.0029, batch acc 0.9212
20:40:17.745      Training iter 250, batch loss 0.0030, batch acc 0.9150
20:40:17.999      Training iter 300, batch loss 0.0032, batch acc 0.9022
20:40:18.259      Training iter 350, batch loss 0.0031, batch acc 0.9090
20:40:18.520      Training iter 400, batch loss 0.0031, batch acc 0.9110
20:40:18.776      Training iter 450, batch loss 0.0031, batch acc 0.9106
20:40:19.031      Training iter 500, batch loss 0.0028, batch acc 0.9184
20:40:19.289      Training iter 550, batch loss 0.0027, batch acc 0.9212
20:40:19.547      Training iter 600, batch loss 0.0028, batch acc 0.9138
20:40:19.549 Testing @ 1 epoch...
20:40:19.740      Testing, total mean loss 0.00265, total acc 0.92210
20:40:20.019      Training iter 50, batch loss 0.0026, batch acc 0.9224
20:40:20.293      Training iter 100, batch loss 0.0028, batch acc 0.9160
20:40:20.489      Training iter 150, batch loss 0.0027, batch acc 0.9202
20:40:20.615      Training iter 200, batch loss 0.0026, batch acc 0.9200
20:40:20.755      Training iter 250, batch loss 0.0025, batch acc 0.9304
20:40:20.907      Training iter 300, batch loss 0.0025, batch acc 0.9300
20:40:21.098      Training iter 350, batch loss 0.0025, batch acc 0.9236
20:40:21.318      Training iter 400, batch loss 0.0027, batch acc 0.9226
20:40:21.457      Training iter 450, batch loss 0.0027, batch acc 0.9194
20:40:21.601      Training iter 500, batch loss 0.0024, batch acc 0.9312
20:40:21.819      Training iter 550, batch loss 0.0026, batch acc 0.9278
20:40:22.034      Training iter 600, batch loss 0.0024, batch acc 0.9298
20:40:22.036 Testing @ 2 epoch...
20:40:22.151      Testing, total mean loss 0.00237, total acc 0.93310
20:40:22.346      Training iter 50, batch loss 0.0023, batch acc 0.9322
20:40:22.567      Training iter 100, batch loss 0.0024, batch acc 0.9302
20:40:22.721      Training iter 150, batch loss 0.0022, batch acc 0.9350
20:40:22.913      Training iter 200, batch loss 0.0022, batch acc 0.9370
20:40:23.161      Training iter 250, batch loss 0.0024, batch acc 0.9332
20:40:23.313      Training iter 300, batch loss 0.0021, batch acc 0.9356
20:40:23.457      Training iter 350, batch loss 0.0023, batch acc 0.9372
20:40:23.632      Training iter 400, batch loss 0.0023, batch acc 0.9362
20:40:23.837      Training iter 450, batch loss 0.0022, batch acc 0.9364
20:40:23.993      Training iter 500, batch loss 0.0024, batch acc 0.9264
20:40:24.191      Training iter 550, batch loss 0.0023, batch acc 0.9358
20:40:24.340      Training iter 600, batch loss 0.0021, batch acc 0.9402
20:40:24.341 Testing @ 3 epoch...
20:40:24.458      Testing, total mean loss 0.00208, total acc 0.93760
20:40:24.627      Training iter 50, batch loss 0.0021, batch acc 0.9392
20:40:24.782      Training iter 100, batch loss 0.0022, batch acc 0.9386
20:40:24.940      Training iter 150, batch loss 0.0019, batch acc 0.9440
20:40:25.155      Training iter 200, batch loss 0.0019, batch acc 0.9438
20:40:25.298      Training iter 250, batch loss 0.0019, batch acc 0.9418
20:40:25.427      Training iter 300, batch loss 0.0020, batch acc 0.9494
20:40:25.625      Training iter 350, batch loss 0.0021, batch acc 0.9446
20:40:25.781      Training iter 400, batch loss 0.0020, batch acc 0.9446
20:40:25.983      Training iter 450, batch loss 0.0021, batch acc 0.9390
20:40:26.198      Training iter 500, batch loss 0.0019, batch acc 0.9450
20:40:26.428      Training iter 550, batch loss 0.0018, batch acc 0.9488
20:40:26.588      Training iter 600, batch loss 0.0019, batch acc 0.9446
20:40:26.590 Testing @ 4 epoch...
20:40:26.682      Testing, total mean loss 0.00183, total acc 0.94660
20:40:26.907      Training iter 50, batch loss 0.0017, batch acc 0.9500
20:40:27.152      Training iter 100, batch loss 0.0018, batch acc 0.9468
20:40:27.387      Training iter 150, batch loss 0.0019, batch acc 0.9470
20:40:27.596      Training iter 200, batch loss 0.0017, batch acc 0.9520
20:40:27.815      Training iter 250, batch loss 0.0020, batch acc 0.9462
20:40:28.008      Training iter 300, batch loss 0.0017, batch acc 0.9518
20:40:28.172      Training iter 350, batch loss 0.0017, batch acc 0.9520
20:40:28.358      Training iter 400, batch loss 0.0018, batch acc 0.9528
20:40:28.553      Training iter 450, batch loss 0.0018, batch acc 0.9472
20:40:28.744      Training iter 500, batch loss 0.0016, batch acc 0.9498
20:40:28.947      Training iter 550, batch loss 0.0017, batch acc 0.9476
20:40:29.160      Training iter 600, batch loss 0.0016, batch acc 0.9546
20:40:29.161 Testing @ 5 epoch...
20:40:29.283      Testing, total mean loss 0.00164, total acc 0.95200
20:40:29.455      Training iter 50, batch loss 0.0015, batch acc 0.9546
20:40:29.673      Training iter 100, batch loss 0.0016, batch acc 0.9536
20:40:29.870      Training iter 150, batch loss 0.0018, batch acc 0.9506
20:40:30.071      Training iter 200, batch loss 0.0016, batch acc 0.9526
20:40:30.288      Training iter 250, batch loss 0.0014, batch acc 0.9602
20:40:30.493      Training iter 300, batch loss 0.0017, batch acc 0.9536
20:40:30.703      Training iter 350, batch loss 0.0015, batch acc 0.9550
20:40:30.912      Training iter 400, batch loss 0.0015, batch acc 0.9548
20:40:31.127      Training iter 450, batch loss 0.0016, batch acc 0.9544
20:40:31.347      Training iter 500, batch loss 0.0015, batch acc 0.9572
20:40:31.546      Training iter 550, batch loss 0.0017, batch acc 0.9530
20:40:31.763      Training iter 600, batch loss 0.0016, batch acc 0.9544
20:40:31.765 Testing @ 6 epoch...
20:40:31.905      Testing, total mean loss 0.00154, total acc 0.95550
20:40:32.125      Training iter 50, batch loss 0.0016, batch acc 0.9516
20:40:32.332      Training iter 100, batch loss 0.0014, batch acc 0.9636
20:40:32.510      Training iter 150, batch loss 0.0014, batch acc 0.9580
20:40:32.693      Training iter 200, batch loss 0.0016, batch acc 0.9554
20:40:32.879      Training iter 250, batch loss 0.0016, batch acc 0.9544
20:40:33.065      Training iter 300, batch loss 0.0014, batch acc 0.9640
20:40:33.246      Training iter 350, batch loss 0.0014, batch acc 0.9590
20:40:33.440      Training iter 400, batch loss 0.0014, batch acc 0.9596
20:40:33.634      Training iter 450, batch loss 0.0014, batch acc 0.9608
20:40:33.827      Training iter 500, batch loss 0.0015, batch acc 0.9578
20:40:34.005      Training iter 550, batch loss 0.0013, batch acc 0.9616
20:40:34.179      Training iter 600, batch loss 0.0013, batch acc 0.9630
20:40:34.180 Testing @ 7 epoch...
20:40:34.320      Testing, total mean loss 0.00140, total acc 0.95830
20:40:34.525      Training iter 50, batch loss 0.0013, batch acc 0.9592
20:40:34.726      Training iter 100, batch loss 0.0014, batch acc 0.9594
20:40:34.927      Training iter 150, batch loss 0.0013, batch acc 0.9632
20:40:35.131      Training iter 200, batch loss 0.0014, batch acc 0.9628
20:40:35.328      Training iter 250, batch loss 0.0014, batch acc 0.9634
20:40:35.530      Training iter 300, batch loss 0.0013, batch acc 0.9616
20:40:35.727      Training iter 350, batch loss 0.0012, batch acc 0.9680
20:40:35.878      Training iter 400, batch loss 0.0013, batch acc 0.9644
20:40:36.004      Training iter 450, batch loss 0.0012, batch acc 0.9646
20:40:36.130      Training iter 500, batch loss 0.0013, batch acc 0.9616
20:40:36.278      Training iter 550, batch loss 0.0012, batch acc 0.9640
20:40:36.452      Training iter 600, batch loss 0.0014, batch acc 0.9606
20:40:36.454 Testing @ 8 epoch...
20:40:36.543      Testing, total mean loss 0.00131, total acc 0.96180
20:40:36.739      Training iter 50, batch loss 0.0012, batch acc 0.9660
20:40:36.867      Training iter 100, batch loss 0.0013, batch acc 0.9638
20:40:37.001      Training iter 150, batch loss 0.0012, batch acc 0.9682
20:40:37.157      Training iter 200, batch loss 0.0012, batch acc 0.9640
20:40:37.318      Training iter 250, batch loss 0.0011, batch acc 0.9696
20:40:37.495      Training iter 300, batch loss 0.0011, batch acc 0.9698
20:40:37.659      Training iter 350, batch loss 0.0013, batch acc 0.9636
20:40:37.828      Training iter 400, batch loss 0.0013, batch acc 0.9622
20:40:38.021      Training iter 450, batch loss 0.0011, batch acc 0.9666
20:40:38.224      Training iter 500, batch loss 0.0012, batch acc 0.9660
20:40:38.394      Training iter 550, batch loss 0.0012, batch acc 0.9652
20:40:38.593      Training iter 600, batch loss 0.0012, batch acc 0.9656
20:40:38.594 Testing @ 9 epoch...
20:40:38.687      Testing, total mean loss 0.00123, total acc 0.96290
20:40:38.882      Training iter 50, batch loss 0.0012, batch acc 0.9700
20:40:39.093      Training iter 100, batch loss 0.0011, batch acc 0.9696
20:40:39.273      Training iter 150, batch loss 0.0012, batch acc 0.9690
20:40:39.470      Training iter 200, batch loss 0.0011, batch acc 0.9660
20:40:39.671      Training iter 250, batch loss 0.0011, batch acc 0.9704
20:40:39.858      Training iter 300, batch loss 0.0011, batch acc 0.9688
20:40:40.086      Training iter 350, batch loss 0.0012, batch acc 0.9652
20:40:40.274      Training iter 400, batch loss 0.0011, batch acc 0.9668
20:40:40.474      Training iter 450, batch loss 0.0011, batch acc 0.9704
20:40:40.661      Training iter 500, batch loss 0.0010, batch acc 0.9700
20:40:40.841      Training iter 550, batch loss 0.0012, batch acc 0.9706
20:40:41.024      Training iter 600, batch loss 0.0011, batch acc 0.9666
20:40:41.025 Testing @ 10 epoch...
20:40:41.123      Testing, total mean loss 0.00117, total acc 0.96530
20:40:41.325      Training iter 50, batch loss 0.0010, batch acc 0.9750
20:40:41.532      Training iter 100, batch loss 0.0010, batch acc 0.9720
20:40:41.697      Training iter 150, batch loss 0.0010, batch acc 0.9718
20:40:41.861      Training iter 200, batch loss 0.0009, batch acc 0.9744
20:40:42.043      Training iter 250, batch loss 0.0011, batch acc 0.9702
20:40:42.243      Training iter 300, batch loss 0.0010, batch acc 0.9738
20:40:42.437      Training iter 350, batch loss 0.0011, batch acc 0.9694
20:40:42.632      Training iter 400, batch loss 0.0010, batch acc 0.9694
20:40:42.819      Training iter 450, batch loss 0.0010, batch acc 0.9718
20:40:43.002      Training iter 500, batch loss 0.0012, batch acc 0.9660
20:40:43.201      Training iter 550, batch loss 0.0010, batch acc 0.9722
20:40:43.373      Training iter 600, batch loss 0.0010, batch acc 0.9678
20:40:43.375 Testing @ 11 epoch...
20:40:43.492      Testing, total mean loss 0.00112, total acc 0.96770
20:40:43.688      Training iter 50, batch loss 0.0011, batch acc 0.9684
20:40:43.834      Training iter 100, batch loss 0.0009, batch acc 0.9724
20:40:43.988      Training iter 150, batch loss 0.0010, batch acc 0.9732
20:40:44.153      Training iter 200, batch loss 0.0008, batch acc 0.9762
20:40:44.328      Training iter 250, batch loss 0.0010, batch acc 0.9718
20:40:44.533      Training iter 300, batch loss 0.0010, batch acc 0.9710
20:40:44.773      Training iter 350, batch loss 0.0009, batch acc 0.9766
20:40:45.014      Training iter 400, batch loss 0.0010, batch acc 0.9712
20:40:45.242      Training iter 450, batch loss 0.0009, batch acc 0.9770
20:40:45.482      Training iter 500, batch loss 0.0010, batch acc 0.9734
20:40:45.731      Training iter 550, batch loss 0.0010, batch acc 0.9682
20:40:45.986      Training iter 600, batch loss 0.0010, batch acc 0.9746
20:40:45.988 Testing @ 12 epoch...
20:40:46.154      Testing, total mean loss 0.00111, total acc 0.96750
20:40:46.392      Training iter 50, batch loss 0.0008, batch acc 0.9774
20:40:46.632      Training iter 100, batch loss 0.0009, batch acc 0.9750
20:40:46.874      Training iter 150, batch loss 0.0009, batch acc 0.9742
20:40:47.106      Training iter 200, batch loss 0.0010, batch acc 0.9728
20:40:47.355      Training iter 250, batch loss 0.0009, batch acc 0.9732
20:40:47.602      Training iter 300, batch loss 0.0009, batch acc 0.9716
20:40:47.837      Training iter 350, batch loss 0.0010, batch acc 0.9730
20:40:48.075      Training iter 400, batch loss 0.0009, batch acc 0.9736
20:40:48.305      Training iter 450, batch loss 0.0009, batch acc 0.9744
20:40:48.539      Training iter 500, batch loss 0.0008, batch acc 0.9762
20:40:48.786      Training iter 550, batch loss 0.0008, batch acc 0.9770
20:40:49.029      Training iter 600, batch loss 0.0009, batch acc 0.9760
20:40:49.031 Testing @ 13 epoch...
20:40:49.212      Testing, total mean loss 0.00102, total acc 0.96920
20:40:49.400      Training iter 50, batch loss 0.0009, batch acc 0.9744
20:40:49.518      Training iter 100, batch loss 0.0008, batch acc 0.9746
20:40:49.624      Training iter 150, batch loss 0.0009, batch acc 0.9756
20:40:49.729      Training iter 200, batch loss 0.0008, batch acc 0.9770
20:40:49.843      Training iter 250, batch loss 0.0009, batch acc 0.9774
20:40:50.039      Training iter 300, batch loss 0.0009, batch acc 0.9762
20:40:50.229      Training iter 350, batch loss 0.0009, batch acc 0.9746
20:40:50.475      Training iter 400, batch loss 0.0009, batch acc 0.9730
20:40:50.715      Training iter 450, batch loss 0.0008, batch acc 0.9770
20:40:50.967      Training iter 500, batch loss 0.0008, batch acc 0.9750
20:40:51.186      Training iter 550, batch loss 0.0009, batch acc 0.9742
20:40:51.434      Training iter 600, batch loss 0.0008, batch acc 0.9768
20:40:51.435 Testing @ 14 epoch...
20:40:51.626      Testing, total mean loss 0.00097, total acc 0.96970
20:40:51.868      Training iter 50, batch loss 0.0008, batch acc 0.9786
20:40:52.111      Training iter 100, batch loss 0.0008, batch acc 0.9780
20:40:52.332      Training iter 150, batch loss 0.0008, batch acc 0.9792
20:40:52.583      Training iter 200, batch loss 0.0008, batch acc 0.9794
20:40:52.824      Training iter 250, batch loss 0.0008, batch acc 0.9772
20:40:53.074      Training iter 300, batch loss 0.0008, batch acc 0.9762
20:40:53.322      Training iter 350, batch loss 0.0008, batch acc 0.9766
20:40:53.568      Training iter 400, batch loss 0.0009, batch acc 0.9764
20:40:53.819      Training iter 450, batch loss 0.0008, batch acc 0.9806
20:40:54.075      Training iter 500, batch loss 0.0008, batch acc 0.9770
20:40:54.314      Training iter 550, batch loss 0.0008, batch acc 0.9768
20:40:54.556      Training iter 600, batch loss 0.0007, batch acc 0.9790
20:40:54.558 Testing @ 15 epoch...
20:40:54.740      Testing, total mean loss 0.00099, total acc 0.97010
20:40:54.976      Training iter 50, batch loss 0.0007, batch acc 0.9810
20:40:55.221      Training iter 100, batch loss 0.0007, batch acc 0.9824
20:40:55.455      Training iter 150, batch loss 0.0008, batch acc 0.9790
20:40:55.698      Training iter 200, batch loss 0.0008, batch acc 0.9756
20:40:55.947      Training iter 250, batch loss 0.0007, batch acc 0.9802
20:40:56.195      Training iter 300, batch loss 0.0008, batch acc 0.9782
20:40:56.435      Training iter 350, batch loss 0.0007, batch acc 0.9812
20:40:56.666      Training iter 400, batch loss 0.0008, batch acc 0.9762
20:40:56.901      Training iter 450, batch loss 0.0007, batch acc 0.9798
20:40:57.148      Training iter 500, batch loss 0.0006, batch acc 0.9820
20:40:57.382      Training iter 550, batch loss 0.0008, batch acc 0.9756
20:40:57.628      Training iter 600, batch loss 0.0009, batch acc 0.9770
20:40:57.630 Testing @ 16 epoch...
20:40:57.823      Testing, total mean loss 0.00092, total acc 0.97190
20:40:58.078      Training iter 50, batch loss 0.0007, batch acc 0.9782
20:40:58.332      Training iter 100, batch loss 0.0007, batch acc 0.9786
20:40:58.547      Training iter 150, batch loss 0.0007, batch acc 0.9798
20:40:58.808      Training iter 200, batch loss 0.0008, batch acc 0.9788
20:40:59.067      Training iter 250, batch loss 0.0008, batch acc 0.9764
20:40:59.319      Training iter 300, batch loss 0.0007, batch acc 0.9806
20:40:59.544      Training iter 350, batch loss 0.0007, batch acc 0.9822
20:40:59.785      Training iter 400, batch loss 0.0006, batch acc 0.9824
20:41:00.027      Training iter 450, batch loss 0.0007, batch acc 0.9784
20:41:00.277      Training iter 500, batch loss 0.0007, batch acc 0.9822
20:41:00.506      Training iter 550, batch loss 0.0007, batch acc 0.9792
20:41:00.760      Training iter 600, batch loss 0.0007, batch acc 0.9816
20:41:00.761 Testing @ 17 epoch...
20:41:00.942      Testing, total mean loss 0.00091, total acc 0.97330
20:41:01.191      Training iter 50, batch loss 0.0007, batch acc 0.9780
20:41:01.438      Training iter 100, batch loss 0.0007, batch acc 0.9812
20:41:01.685      Training iter 150, batch loss 0.0006, batch acc 0.9826
20:41:01.945      Training iter 200, batch loss 0.0006, batch acc 0.9850
20:41:02.210      Training iter 250, batch loss 0.0007, batch acc 0.9798
20:41:02.464      Training iter 300, batch loss 0.0007, batch acc 0.9804
20:41:02.715      Training iter 350, batch loss 0.0007, batch acc 0.9788
20:41:02.960      Training iter 400, batch loss 0.0006, batch acc 0.9820
20:41:03.214      Training iter 450, batch loss 0.0006, batch acc 0.9822
20:41:03.456      Training iter 500, batch loss 0.0008, batch acc 0.9782
20:41:03.692      Training iter 550, batch loss 0.0007, batch acc 0.9818
20:41:03.927      Training iter 600, batch loss 0.0007, batch acc 0.9798
20:41:03.929 Testing @ 18 epoch...
20:41:04.122      Testing, total mean loss 0.00089, total acc 0.97280
20:41:04.371      Training iter 50, batch loss 0.0007, batch acc 0.9824
20:41:04.609      Training iter 100, batch loss 0.0006, batch acc 0.9838
20:41:04.832      Training iter 150, batch loss 0.0006, batch acc 0.9808
20:41:05.088      Training iter 200, batch loss 0.0006, batch acc 0.9820
20:41:05.338      Training iter 250, batch loss 0.0006, batch acc 0.9848
20:41:05.596      Training iter 300, batch loss 0.0007, batch acc 0.9794
20:41:05.827      Training iter 350, batch loss 0.0006, batch acc 0.9826
20:41:06.046      Training iter 400, batch loss 0.0007, batch acc 0.9822
20:41:06.277      Training iter 450, batch loss 0.0007, batch acc 0.9812
20:41:06.500      Training iter 500, batch loss 0.0007, batch acc 0.9804
20:41:06.711      Training iter 550, batch loss 0.0006, batch acc 0.9820
20:41:06.928      Training iter 600, batch loss 0.0006, batch acc 0.9838
20:41:06.930 Testing @ 19 epoch...
20:41:07.069      Testing, total mean loss 0.00086, total acc 0.97300
20:41:07.283      Training iter 50, batch loss 0.0007, batch acc 0.9808
20:41:07.523      Training iter 100, batch loss 0.0006, batch acc 0.9840
20:41:07.769      Training iter 150, batch loss 0.0006, batch acc 0.9812
20:41:08.015      Training iter 200, batch loss 0.0006, batch acc 0.9852
20:41:08.250      Training iter 250, batch loss 0.0007, batch acc 0.9822
20:41:08.493      Training iter 300, batch loss 0.0006, batch acc 0.9842
20:41:08.725      Training iter 350, batch loss 0.0006, batch acc 0.9808
20:41:08.958      Training iter 400, batch loss 0.0006, batch acc 0.9838
20:41:09.198      Training iter 450, batch loss 0.0006, batch acc 0.9808
20:41:09.452      Training iter 500, batch loss 0.0006, batch acc 0.9832
20:41:09.701      Training iter 550, batch loss 0.0005, batch acc 0.9858
20:41:09.931      Training iter 600, batch loss 0.0006, batch acc 0.9822
20:41:09.933 Testing @ 20 epoch...
20:41:10.111      Testing, total mean loss 0.00086, total acc 0.97300
20:41:10.338      Training iter 50, batch loss 0.0006, batch acc 0.9820
20:41:10.590      Training iter 100, batch loss 0.0006, batch acc 0.9834
20:41:10.829      Training iter 150, batch loss 0.0006, batch acc 0.9856
20:41:11.081      Training iter 200, batch loss 0.0006, batch acc 0.9850
20:41:11.329      Training iter 250, batch loss 0.0005, batch acc 0.9852
20:41:11.557      Training iter 300, batch loss 0.0006, batch acc 0.9866
20:41:11.807      Training iter 350, batch loss 0.0006, batch acc 0.9858
20:41:12.048      Training iter 400, batch loss 0.0005, batch acc 0.9850
20:41:12.297      Training iter 450, batch loss 0.0006, batch acc 0.9852
20:41:12.534      Training iter 500, batch loss 0.0007, batch acc 0.9794
20:41:12.777      Training iter 550, batch loss 0.0005, batch acc 0.9846
20:41:13.011      Training iter 600, batch loss 0.0006, batch acc 0.9842
20:41:13.012 Testing @ 21 epoch...
20:41:13.194      Testing, total mean loss 0.00084, total acc 0.97480
20:41:13.457      Training iter 50, batch loss 0.0005, batch acc 0.9878
20:41:13.714      Training iter 100, batch loss 0.0006, batch acc 0.9824
20:41:13.963      Training iter 150, batch loss 0.0006, batch acc 0.9842
20:41:14.220      Training iter 200, batch loss 0.0006, batch acc 0.9850
20:41:14.466      Training iter 250, batch loss 0.0006, batch acc 0.9858
20:41:14.710      Training iter 300, batch loss 0.0006, batch acc 0.9830
20:41:14.958      Training iter 350, batch loss 0.0006, batch acc 0.9834
20:41:15.208      Training iter 400, batch loss 0.0005, batch acc 0.9850
20:41:15.453      Training iter 450, batch loss 0.0005, batch acc 0.9860
20:41:15.709      Training iter 500, batch loss 0.0006, batch acc 0.9828
20:41:15.941      Training iter 550, batch loss 0.0005, batch acc 0.9850
20:41:16.187      Training iter 600, batch loss 0.0005, batch acc 0.9850
20:41:16.189 Testing @ 22 epoch...
20:41:16.382      Testing, total mean loss 0.00087, total acc 0.97350
20:41:16.641      Training iter 50, batch loss 0.0006, batch acc 0.9848
20:41:16.897      Training iter 100, batch loss 0.0005, batch acc 0.9876
20:41:17.127      Training iter 150, batch loss 0.0005, batch acc 0.9884
20:41:17.351      Training iter 200, batch loss 0.0005, batch acc 0.9874
20:41:17.626      Training iter 250, batch loss 0.0005, batch acc 0.9882
20:41:17.902      Training iter 300, batch loss 0.0006, batch acc 0.9852
20:41:18.163      Training iter 350, batch loss 0.0006, batch acc 0.9836
20:41:18.432      Training iter 400, batch loss 0.0005, batch acc 0.9854
20:41:18.701      Training iter 450, batch loss 0.0005, batch acc 0.9848
20:41:18.967      Training iter 500, batch loss 0.0006, batch acc 0.9844
20:41:19.223      Training iter 550, batch loss 0.0005, batch acc 0.9828
20:41:19.470      Training iter 600, batch loss 0.0005, batch acc 0.9838
20:41:19.472 Testing @ 23 epoch...
20:41:19.593      Testing, total mean loss 0.00081, total acc 0.97490
20:41:19.801      Training iter 50, batch loss 0.0005, batch acc 0.9886
20:41:20.004      Training iter 100, batch loss 0.0005, batch acc 0.9868
20:41:20.208      Training iter 150, batch loss 0.0005, batch acc 0.9858
20:41:20.436      Training iter 200, batch loss 0.0005, batch acc 0.9844
20:41:20.666      Training iter 250, batch loss 0.0006, batch acc 0.9838
20:41:20.889      Training iter 300, batch loss 0.0005, batch acc 0.9862
20:41:21.151      Training iter 350, batch loss 0.0005, batch acc 0.9852
20:41:21.425      Training iter 400, batch loss 0.0005, batch acc 0.9864
20:41:21.708      Training iter 450, batch loss 0.0005, batch acc 0.9876
20:41:22.002      Training iter 500, batch loss 0.0005, batch acc 0.9882
20:41:22.286      Training iter 550, batch loss 0.0005, batch acc 0.9844
20:41:22.561      Training iter 600, batch loss 0.0005, batch acc 0.9846
20:41:22.563 Testing @ 24 epoch...
20:41:22.716      Testing, total mean loss 0.00079, total acc 0.97510
20:41:22.989      Training iter 50, batch loss 0.0005, batch acc 0.9896
20:41:23.251      Training iter 100, batch loss 0.0005, batch acc 0.9866
20:41:23.512      Training iter 150, batch loss 0.0004, batch acc 0.9892
20:41:23.788      Training iter 200, batch loss 0.0005, batch acc 0.9868
20:41:24.061      Training iter 250, batch loss 0.0005, batch acc 0.9844
20:41:24.314      Training iter 300, batch loss 0.0005, batch acc 0.9888
20:41:24.580      Training iter 350, batch loss 0.0004, batch acc 0.9886
20:41:24.853      Training iter 400, batch loss 0.0005, batch acc 0.9854
20:41:25.131      Training iter 450, batch loss 0.0005, batch acc 0.9852
20:41:25.402      Training iter 500, batch loss 0.0005, batch acc 0.9842
20:41:25.684      Training iter 550, batch loss 0.0005, batch acc 0.9866
20:41:25.959      Training iter 600, batch loss 0.0005, batch acc 0.9902
20:41:25.961 Testing @ 25 epoch...
20:41:26.141      Testing, total mean loss 0.00078, total acc 0.97550
20:41:26.408      Training iter 50, batch loss 0.0004, batch acc 0.9904
20:41:26.672      Training iter 100, batch loss 0.0005, batch acc 0.9886
20:41:26.916      Training iter 150, batch loss 0.0005, batch acc 0.9876
20:41:27.163      Training iter 200, batch loss 0.0005, batch acc 0.9892
20:41:27.429      Training iter 250, batch loss 0.0005, batch acc 0.9876
20:41:27.685      Training iter 300, batch loss 0.0004, batch acc 0.9894
20:41:27.911      Training iter 350, batch loss 0.0004, batch acc 0.9886
20:41:28.165      Training iter 400, batch loss 0.0004, batch acc 0.9876
20:41:28.384      Training iter 450, batch loss 0.0005, batch acc 0.9886
20:41:28.627      Training iter 500, batch loss 0.0006, batch acc 0.9844
20:41:28.880      Training iter 550, batch loss 0.0005, batch acc 0.9892
20:41:29.144      Training iter 600, batch loss 0.0005, batch acc 0.9870
20:41:29.145 Testing @ 26 epoch...
20:41:29.336      Testing, total mean loss 0.00077, total acc 0.97570
20:41:29.609      Training iter 50, batch loss 0.0004, batch acc 0.9876
20:41:29.888      Training iter 100, batch loss 0.0004, batch acc 0.9916
20:41:30.159      Training iter 150, batch loss 0.0004, batch acc 0.9902
20:41:30.428      Training iter 200, batch loss 0.0004, batch acc 0.9906
20:41:30.696      Training iter 250, batch loss 0.0005, batch acc 0.9870
20:41:30.963      Training iter 300, batch loss 0.0004, batch acc 0.9894
20:41:31.236      Training iter 350, batch loss 0.0004, batch acc 0.9876
20:41:31.495      Training iter 400, batch loss 0.0005, batch acc 0.9874
20:41:31.770      Training iter 450, batch loss 0.0004, batch acc 0.9866
20:41:32.036      Training iter 500, batch loss 0.0004, batch acc 0.9868
20:41:32.294      Training iter 550, batch loss 0.0005, batch acc 0.9870
20:41:32.557      Training iter 600, batch loss 0.0005, batch acc 0.9856
20:41:32.559 Testing @ 27 epoch...
20:41:32.740      Testing, total mean loss 0.00078, total acc 0.97490
20:41:33.026      Training iter 50, batch loss 0.0004, batch acc 0.9890
20:41:33.310      Training iter 100, batch loss 0.0004, batch acc 0.9868
20:41:33.570      Training iter 150, batch loss 0.0004, batch acc 0.9894
20:41:33.839      Training iter 200, batch loss 0.0004, batch acc 0.9906
20:41:34.131      Training iter 250, batch loss 0.0004, batch acc 0.9864
20:41:34.402      Training iter 300, batch loss 0.0004, batch acc 0.9888
20:41:34.669      Training iter 350, batch loss 0.0005, batch acc 0.9898
20:41:34.910      Training iter 400, batch loss 0.0004, batch acc 0.9902
20:41:35.166      Training iter 450, batch loss 0.0004, batch acc 0.9908
20:41:35.425      Training iter 500, batch loss 0.0004, batch acc 0.9922
20:41:35.644      Training iter 550, batch loss 0.0004, batch acc 0.9878
20:41:35.861      Training iter 600, batch loss 0.0005, batch acc 0.9882
20:41:35.863 Testing @ 28 epoch...
20:41:35.979      Testing, total mean loss 0.00076, total acc 0.97540
20:41:36.207      Training iter 50, batch loss 0.0005, batch acc 0.9876
20:41:36.449      Training iter 100, batch loss 0.0004, batch acc 0.9898
20:41:36.699      Training iter 150, batch loss 0.0004, batch acc 0.9896
20:41:36.926      Training iter 200, batch loss 0.0004, batch acc 0.9924
20:41:37.160      Training iter 250, batch loss 0.0004, batch acc 0.9904
20:41:37.397      Training iter 300, batch loss 0.0004, batch acc 0.9906
20:41:37.568      Training iter 350, batch loss 0.0004, batch acc 0.9866
20:41:37.726      Training iter 400, batch loss 0.0004, batch acc 0.9894
20:41:37.892      Training iter 450, batch loss 0.0004, batch acc 0.9892
20:41:38.096      Training iter 500, batch loss 0.0004, batch acc 0.9910
20:41:38.288      Training iter 550, batch loss 0.0005, batch acc 0.9874
20:41:38.485      Training iter 600, batch loss 0.0004, batch acc 0.9906
20:41:38.487 Testing @ 29 epoch...
20:41:38.594      Testing, total mean loss 0.00077, total acc 0.97510
20:41:38.764      Training iter 50, batch loss 0.0005, batch acc 0.9872
20:41:38.909      Training iter 100, batch loss 0.0003, batch acc 0.9918
20:41:39.100      Training iter 150, batch loss 0.0004, batch acc 0.9886
20:41:39.316      Training iter 200, batch loss 0.0003, batch acc 0.9928
20:41:39.485      Training iter 250, batch loss 0.0004, batch acc 0.9886
20:41:39.639      Training iter 300, batch loss 0.0004, batch acc 0.9902
20:41:39.774      Training iter 350, batch loss 0.0003, batch acc 0.9918
20:41:39.888      Training iter 400, batch loss 0.0004, batch acc 0.9902
20:41:40.010      Training iter 450, batch loss 0.0004, batch acc 0.9890
20:41:40.145      Training iter 500, batch loss 0.0004, batch acc 0.9892
20:41:40.284      Training iter 550, batch loss 0.0004, batch acc 0.9898
20:41:40.428      Training iter 600, batch loss 0.0004, batch acc 0.9878
20:41:40.430 Testing @ 30 epoch...
20:41:40.517      Testing, total mean loss 0.00075, total acc 0.97630
20:41:40.674      Training iter 50, batch loss 0.0004, batch acc 0.9916
20:41:40.878      Training iter 100, batch loss 0.0003, batch acc 0.9942
20:41:41.027      Training iter 150, batch loss 0.0004, batch acc 0.9894
20:41:41.159      Training iter 200, batch loss 0.0004, batch acc 0.9900
20:41:41.312      Training iter 250, batch loss 0.0004, batch acc 0.9904
20:41:41.449      Training iter 300, batch loss 0.0003, batch acc 0.9918
20:41:41.697      Training iter 350, batch loss 0.0004, batch acc 0.9878
20:41:41.942      Training iter 400, batch loss 0.0004, batch acc 0.9912
20:41:42.192      Training iter 450, batch loss 0.0004, batch acc 0.9908
20:41:42.451      Training iter 500, batch loss 0.0004, batch acc 0.9908
20:41:42.687      Training iter 550, batch loss 0.0004, batch acc 0.9890
20:41:42.930      Training iter 600, batch loss 0.0004, batch acc 0.9872
20:41:42.935 Testing @ 31 epoch...
20:41:43.123      Testing, total mean loss 0.00071, total acc 0.97700
20:41:43.366      Training iter 50, batch loss 0.0003, batch acc 0.9938
20:41:43.664      Training iter 100, batch loss 0.0004, batch acc 0.9910
20:41:44.236      Training iter 150, batch loss 0.0003, batch acc 0.9916
20:41:44.813      Training iter 200, batch loss 0.0004, batch acc 0.9914
20:41:45.395      Training iter 250, batch loss 0.0003, batch acc 0.9916
20:41:45.969      Training iter 300, batch loss 0.0003, batch acc 0.9920
20:41:46.510      Training iter 350, batch loss 0.0004, batch acc 0.9906
20:41:47.071      Training iter 400, batch loss 0.0004, batch acc 0.9890
20:41:47.636      Training iter 450, batch loss 0.0004, batch acc 0.9904
20:41:48.202      Training iter 500, batch loss 0.0004, batch acc 0.9902
20:41:48.694      Training iter 550, batch loss 0.0004, batch acc 0.9896
20:41:49.192      Training iter 600, batch loss 0.0004, batch acc 0.9894
20:41:49.204 Testing @ 32 epoch...
20:41:49.644      Testing, total mean loss 0.00073, total acc 0.97670
20:41:50.178      Training iter 50, batch loss 0.0003, batch acc 0.9920
20:41:50.637      Training iter 100, batch loss 0.0004, batch acc 0.9916
20:41:51.072      Training iter 150, batch loss 0.0003, batch acc 0.9916
20:41:51.482      Training iter 200, batch loss 0.0003, batch acc 0.9924
20:41:52.012      Training iter 250, batch loss 0.0003, batch acc 0.9920
20:41:52.466      Training iter 300, batch loss 0.0004, batch acc 0.9896
20:41:52.976      Training iter 350, batch loss 0.0004, batch acc 0.9898
20:41:53.383      Training iter 400, batch loss 0.0003, batch acc 0.9936
20:41:53.697      Training iter 450, batch loss 0.0004, batch acc 0.9898
20:41:54.118      Training iter 500, batch loss 0.0003, batch acc 0.9934
20:41:54.466      Training iter 550, batch loss 0.0004, batch acc 0.9916
20:41:54.959      Training iter 600, batch loss 0.0003, batch acc 0.9908
20:41:54.969 Testing @ 33 epoch...
20:41:55.422      Testing, total mean loss 0.00076, total acc 0.97490
20:41:55.967      Training iter 50, batch loss 0.0003, batch acc 0.9930
20:41:56.526      Training iter 100, batch loss 0.0004, batch acc 0.9920
20:41:57.052      Training iter 150, batch loss 0.0004, batch acc 0.9900
20:41:57.303      Training iter 200, batch loss 0.0003, batch acc 0.9938
20:41:57.560      Training iter 250, batch loss 0.0003, batch acc 0.9932
20:41:57.813      Training iter 300, batch loss 0.0003, batch acc 0.9910
20:41:58.102      Training iter 350, batch loss 0.0003, batch acc 0.9934
20:41:58.368      Training iter 400, batch loss 0.0003, batch acc 0.9910
20:41:58.603      Training iter 450, batch loss 0.0003, batch acc 0.9918
20:41:58.814      Training iter 500, batch loss 0.0003, batch acc 0.9902
20:41:58.925      Training iter 550, batch loss 0.0003, batch acc 0.9910
20:41:59.045      Training iter 600, batch loss 0.0003, batch acc 0.9920
20:41:59.046 Testing @ 34 epoch...
20:41:59.163      Testing, total mean loss 0.00073, total acc 0.97890
20:41:59.410      Training iter 50, batch loss 0.0003, batch acc 0.9930
20:41:59.668      Training iter 100, batch loss 0.0003, batch acc 0.9930
20:41:59.901      Training iter 150, batch loss 0.0003, batch acc 0.9922
20:42:00.145      Training iter 200, batch loss 0.0003, batch acc 0.9938
20:42:00.399      Training iter 250, batch loss 0.0003, batch acc 0.9938
20:42:00.649      Training iter 300, batch loss 0.0004, batch acc 0.9914
20:42:00.894      Training iter 350, batch loss 0.0004, batch acc 0.9912
20:42:01.137      Training iter 400, batch loss 0.0003, batch acc 0.9920
20:42:01.394      Training iter 450, batch loss 0.0004, batch acc 0.9914
20:42:01.649      Training iter 500, batch loss 0.0003, batch acc 0.9932
20:42:01.888      Training iter 550, batch loss 0.0003, batch acc 0.9910
20:42:02.133      Training iter 600, batch loss 0.0003, batch acc 0.9912
20:42:02.134 Testing @ 35 epoch...
20:42:02.311      Testing, total mean loss 0.00070, total acc 0.97810
20:42:02.564      Training iter 50, batch loss 0.0003, batch acc 0.9942
20:42:02.848      Training iter 100, batch loss 0.0003, batch acc 0.9932
20:42:03.103      Training iter 150, batch loss 0.0003, batch acc 0.9948
20:42:03.368      Training iter 200, batch loss 0.0003, batch acc 0.9936
20:42:03.644      Training iter 250, batch loss 0.0003, batch acc 0.9920
20:42:03.916      Training iter 300, batch loss 0.0003, batch acc 0.9932
20:42:04.142      Training iter 350, batch loss 0.0003, batch acc 0.9916
20:42:04.396      Training iter 400, batch loss 0.0004, batch acc 0.9902
20:42:04.650      Training iter 450, batch loss 0.0003, batch acc 0.9922
20:42:04.905      Training iter 500, batch loss 0.0003, batch acc 0.9932
20:42:05.161      Training iter 550, batch loss 0.0003, batch acc 0.9914
20:42:05.430      Training iter 600, batch loss 0.0003, batch acc 0.9914
20:42:05.437 Testing @ 36 epoch...
20:42:05.625      Testing, total mean loss 0.00072, total acc 0.97690
20:42:05.880      Training iter 50, batch loss 0.0003, batch acc 0.9932
20:42:06.130      Training iter 100, batch loss 0.0003, batch acc 0.9938
20:42:06.394      Training iter 150, batch loss 0.0003, batch acc 0.9928
20:42:06.650      Training iter 200, batch loss 0.0003, batch acc 0.9938
20:42:06.920      Training iter 250, batch loss 0.0003, batch acc 0.9928
20:42:07.171      Training iter 300, batch loss 0.0003, batch acc 0.9918
20:42:07.427      Training iter 350, batch loss 0.0003, batch acc 0.9950
20:42:07.669      Training iter 400, batch loss 0.0004, batch acc 0.9932
20:42:07.912      Training iter 450, batch loss 0.0003, batch acc 0.9950
20:42:08.167      Training iter 500, batch loss 0.0003, batch acc 0.9924
20:42:08.418      Training iter 550, batch loss 0.0003, batch acc 0.9920
20:42:08.680      Training iter 600, batch loss 0.0003, batch acc 0.9936
20:42:08.682 Testing @ 37 epoch...
20:42:08.865      Testing, total mean loss 0.00072, total acc 0.97700
20:42:09.115      Training iter 50, batch loss 0.0003, batch acc 0.9938
20:42:09.365      Training iter 100, batch loss 0.0003, batch acc 0.9930
20:42:09.602      Training iter 150, batch loss 0.0003, batch acc 0.9934
20:42:09.853      Training iter 200, batch loss 0.0003, batch acc 0.9930
20:42:10.133      Training iter 250, batch loss 0.0003, batch acc 0.9914
20:42:10.391      Training iter 300, batch loss 0.0003, batch acc 0.9940
20:42:10.651      Training iter 350, batch loss 0.0004, batch acc 0.9918
20:42:10.914      Training iter 400, batch loss 0.0002, batch acc 0.9960
20:42:11.175      Training iter 450, batch loss 0.0003, batch acc 0.9942
20:42:11.427      Training iter 500, batch loss 0.0003, batch acc 0.9916
20:42:11.697      Training iter 550, batch loss 0.0003, batch acc 0.9950
20:42:11.956      Training iter 600, batch loss 0.0003, batch acc 0.9936
20:42:11.969 Testing @ 38 epoch...
20:42:12.149      Testing, total mean loss 0.00073, total acc 0.97590
20:42:12.401      Training iter 50, batch loss 0.0003, batch acc 0.9922
20:42:12.664      Training iter 100, batch loss 0.0003, batch acc 0.9946
20:42:12.914      Training iter 150, batch loss 0.0003, batch acc 0.9946
20:42:13.168      Training iter 200, batch loss 0.0003, batch acc 0.9922
20:42:13.409      Training iter 250, batch loss 0.0003, batch acc 0.9918
20:42:13.669      Training iter 300, batch loss 0.0002, batch acc 0.9958
20:42:13.920      Training iter 350, batch loss 0.0003, batch acc 0.9934
20:42:14.181      Training iter 400, batch loss 0.0003, batch acc 0.9938
20:42:14.436      Training iter 450, batch loss 0.0003, batch acc 0.9936
20:42:14.695      Training iter 500, batch loss 0.0003, batch acc 0.9932
20:42:14.951      Training iter 550, batch loss 0.0003, batch acc 0.9938
20:42:15.220      Training iter 600, batch loss 0.0003, batch acc 0.9938
20:42:15.226 Testing @ 39 epoch...
20:42:15.423      Testing, total mean loss 0.00069, total acc 0.97870
20:42:15.668      Training iter 50, batch loss 0.0002, batch acc 0.9964
20:42:15.924      Training iter 100, batch loss 0.0003, batch acc 0.9954
20:42:16.193      Training iter 150, batch loss 0.0003, batch acc 0.9932
20:42:16.458      Training iter 200, batch loss 0.0003, batch acc 0.9938
20:42:16.709      Training iter 250, batch loss 0.0003, batch acc 0.9938
20:42:16.984      Training iter 300, batch loss 0.0002, batch acc 0.9942
20:42:17.220      Training iter 350, batch loss 0.0002, batch acc 0.9942
20:42:17.442      Training iter 400, batch loss 0.0003, batch acc 0.9928
20:42:17.708      Training iter 450, batch loss 0.0003, batch acc 0.9936
20:42:17.959      Training iter 500, batch loss 0.0003, batch acc 0.9934
20:42:18.225      Training iter 550, batch loss 0.0002, batch acc 0.9938
20:42:18.492      Training iter 600, batch loss 0.0003, batch acc 0.9956
20:42:18.500 Testing @ 40 epoch...
20:42:18.670      Testing, total mean loss 0.00070, total acc 0.97780
20:42:18.922      Training iter 50, batch loss 0.0003, batch acc 0.9944
20:42:19.173      Training iter 100, batch loss 0.0002, batch acc 0.9942
20:42:19.427      Training iter 150, batch loss 0.0003, batch acc 0.9936
20:42:19.677      Training iter 200, batch loss 0.0002, batch acc 0.9950
20:42:19.936      Training iter 250, batch loss 0.0002, batch acc 0.9972
20:42:20.196      Training iter 300, batch loss 0.0003, batch acc 0.9946
20:42:20.454      Training iter 350, batch loss 0.0003, batch acc 0.9944
20:42:20.692      Training iter 400, batch loss 0.0003, batch acc 0.9932
20:42:20.928      Training iter 450, batch loss 0.0003, batch acc 0.9948
20:42:21.156      Training iter 500, batch loss 0.0002, batch acc 0.9948
20:42:21.388      Training iter 550, batch loss 0.0002, batch acc 0.9940
20:42:21.652      Training iter 600, batch loss 0.0002, batch acc 0.9958
20:42:21.654 Testing @ 41 epoch...
20:42:21.827      Testing, total mean loss 0.00072, total acc 0.97800
20:42:22.094      Training iter 50, batch loss 0.0002, batch acc 0.9942
20:42:22.349      Training iter 100, batch loss 0.0003, batch acc 0.9942
20:42:22.601      Training iter 150, batch loss 0.0002, batch acc 0.9944
20:42:22.795      Training iter 200, batch loss 0.0002, batch acc 0.9968
20:42:23.048      Training iter 250, batch loss 0.0002, batch acc 0.9960
20:42:23.317      Training iter 300, batch loss 0.0003, batch acc 0.9928
20:42:23.589      Training iter 350, batch loss 0.0003, batch acc 0.9950
20:42:23.849      Training iter 400, batch loss 0.0002, batch acc 0.9940
20:42:24.102      Training iter 450, batch loss 0.0003, batch acc 0.9942
20:42:24.386      Training iter 500, batch loss 0.0002, batch acc 0.9948
20:42:24.654      Training iter 550, batch loss 0.0003, batch acc 0.9946
20:42:24.899      Training iter 600, batch loss 0.0002, batch acc 0.9958
20:42:24.912 Testing @ 42 epoch...
20:42:25.100      Testing, total mean loss 0.00071, total acc 0.97780
20:42:25.370      Training iter 50, batch loss 0.0002, batch acc 0.9958
20:42:25.616      Training iter 100, batch loss 0.0002, batch acc 0.9954
20:42:25.894      Training iter 150, batch loss 0.0003, batch acc 0.9942
20:42:26.162      Training iter 200, batch loss 0.0002, batch acc 0.9950
20:42:26.426      Training iter 250, batch loss 0.0002, batch acc 0.9960
20:42:26.686      Training iter 300, batch loss 0.0002, batch acc 0.9952
20:42:26.943      Training iter 350, batch loss 0.0002, batch acc 0.9952
20:42:27.179      Training iter 400, batch loss 0.0002, batch acc 0.9952
20:42:27.446      Training iter 450, batch loss 0.0002, batch acc 0.9946
20:42:27.697      Training iter 500, batch loss 0.0002, batch acc 0.9950
20:42:27.947      Training iter 550, batch loss 0.0003, batch acc 0.9944
20:42:28.195      Training iter 600, batch loss 0.0003, batch acc 0.9928
20:42:28.197 Testing @ 43 epoch...
20:42:28.397      Testing, total mean loss 0.00070, total acc 0.97930
20:42:28.672      Training iter 50, batch loss 0.0002, batch acc 0.9956
20:42:28.926      Training iter 100, batch loss 0.0002, batch acc 0.9954
20:42:29.162      Training iter 150, batch loss 0.0002, batch acc 0.9970
20:42:29.418      Training iter 200, batch loss 0.0002, batch acc 0.9958
20:42:29.666      Training iter 250, batch loss 0.0003, batch acc 0.9946
20:42:29.912      Training iter 300, batch loss 0.0002, batch acc 0.9972
20:42:30.167      Training iter 350, batch loss 0.0002, batch acc 0.9964
20:42:30.426      Training iter 400, batch loss 0.0003, batch acc 0.9940
20:42:30.643      Training iter 450, batch loss 0.0002, batch acc 0.9958
20:42:30.906      Training iter 500, batch loss 0.0002, batch acc 0.9962
20:42:31.159      Training iter 550, batch loss 0.0002, batch acc 0.9960
20:42:31.420      Training iter 600, batch loss 0.0002, batch acc 0.9940
20:42:31.429 Testing @ 44 epoch...
20:42:31.629      Testing, total mean loss 0.00070, total acc 0.97770
20:42:31.894      Training iter 50, batch loss 0.0002, batch acc 0.9972
20:42:32.139      Training iter 100, batch loss 0.0002, batch acc 0.9968
20:42:32.397      Training iter 150, batch loss 0.0002, batch acc 0.9976
20:42:32.662      Training iter 200, batch loss 0.0002, batch acc 0.9956
20:42:32.928      Training iter 250, batch loss 0.0002, batch acc 0.9958
20:42:33.187      Training iter 300, batch loss 0.0003, batch acc 0.9940
20:42:33.445      Training iter 350, batch loss 0.0002, batch acc 0.9952
20:42:33.715      Training iter 400, batch loss 0.0003, batch acc 0.9938
20:42:33.957      Training iter 450, batch loss 0.0002, batch acc 0.9964
20:42:34.207      Training iter 500, batch loss 0.0002, batch acc 0.9964
20:42:34.474      Training iter 550, batch loss 0.0002, batch acc 0.9954
20:42:34.726      Training iter 600, batch loss 0.0003, batch acc 0.9954
20:42:34.739 Testing @ 45 epoch...
20:42:34.880      Testing, total mean loss 0.00070, total acc 0.97710
20:42:35.140      Training iter 50, batch loss 0.0002, batch acc 0.9950
20:42:35.345      Training iter 100, batch loss 0.0002, batch acc 0.9972
20:42:35.555      Training iter 150, batch loss 0.0002, batch acc 0.9970
20:42:35.744      Training iter 200, batch loss 0.0002, batch acc 0.9950
20:42:35.968      Training iter 250, batch loss 0.0002, batch acc 0.9954
20:42:36.201      Training iter 300, batch loss 0.0002, batch acc 0.9944
20:42:36.436      Training iter 350, batch loss 0.0002, batch acc 0.9962
20:42:36.651      Training iter 400, batch loss 0.0002, batch acc 0.9960
20:42:36.883      Training iter 450, batch loss 0.0002, batch acc 0.9966
20:42:37.109      Training iter 500, batch loss 0.0002, batch acc 0.9970
20:42:37.329      Training iter 550, batch loss 0.0002, batch acc 0.9962
20:42:37.542      Training iter 600, batch loss 0.0002, batch acc 0.9956
20:42:37.544 Testing @ 46 epoch...
20:42:37.702      Testing, total mean loss 0.00071, total acc 0.97660
20:42:37.935      Training iter 50, batch loss 0.0002, batch acc 0.9964
20:42:38.165      Training iter 100, batch loss 0.0003, batch acc 0.9952
20:42:38.385      Training iter 150, batch loss 0.0002, batch acc 0.9958
20:42:38.599      Training iter 200, batch loss 0.0002, batch acc 0.9964
20:42:38.821      Training iter 250, batch loss 0.0002, batch acc 0.9980
20:42:39.063      Training iter 300, batch loss 0.0002, batch acc 0.9962
20:42:39.309      Training iter 350, batch loss 0.0002, batch acc 0.9964
20:42:39.545      Training iter 400, batch loss 0.0002, batch acc 0.9964
20:42:39.790      Training iter 450, batch loss 0.0002, batch acc 0.9964
20:42:40.023      Training iter 500, batch loss 0.0002, batch acc 0.9974
20:42:40.240      Training iter 550, batch loss 0.0002, batch acc 0.9958
20:42:40.485      Training iter 600, batch loss 0.0002, batch acc 0.9946
20:42:40.497 Testing @ 47 epoch...
20:42:40.676      Testing, total mean loss 0.00068, total acc 0.97850
20:42:40.926      Training iter 50, batch loss 0.0002, batch acc 0.9962
20:42:41.190      Training iter 100, batch loss 0.0002, batch acc 0.9984
20:42:41.464      Training iter 150, batch loss 0.0002, batch acc 0.9980
20:42:41.704      Training iter 200, batch loss 0.0002, batch acc 0.9968
20:42:41.961      Training iter 250, batch loss 0.0002, batch acc 0.9954
20:42:42.225      Training iter 300, batch loss 0.0002, batch acc 0.9962
20:42:42.485      Training iter 350, batch loss 0.0002, batch acc 0.9970
20:42:42.725      Training iter 400, batch loss 0.0002, batch acc 0.9952
20:42:42.975      Training iter 450, batch loss 0.0002, batch acc 0.9952
20:42:43.227      Training iter 500, batch loss 0.0002, batch acc 0.9952
20:42:43.474      Training iter 550, batch loss 0.0002, batch acc 0.9968
20:42:43.719      Training iter 600, batch loss 0.0002, batch acc 0.9956
20:42:43.727 Testing @ 48 epoch...
20:42:43.905      Testing, total mean loss 0.00069, total acc 0.97810
20:42:44.168      Training iter 50, batch loss 0.0002, batch acc 0.9984
20:42:44.418      Training iter 100, batch loss 0.0002, batch acc 0.9974
20:42:44.644      Training iter 150, batch loss 0.0002, batch acc 0.9978
20:42:44.899      Training iter 200, batch loss 0.0002, batch acc 0.9952
20:42:45.155      Training iter 250, batch loss 0.0002, batch acc 0.9970
20:42:45.411      Training iter 300, batch loss 0.0002, batch acc 0.9960
20:42:45.670      Training iter 350, batch loss 0.0002, batch acc 0.9948
20:42:45.921      Training iter 400, batch loss 0.0002, batch acc 0.9968
20:42:46.194      Training iter 450, batch loss 0.0002, batch acc 0.9970
20:42:46.461      Training iter 500, batch loss 0.0002, batch acc 0.9978
20:42:46.700      Training iter 550, batch loss 0.0002, batch acc 0.9948
20:42:46.970      Training iter 600, batch loss 0.0002, batch acc 0.9976
20:42:46.974 Testing @ 49 epoch...
20:42:47.156      Testing, total mean loss 0.00067, total acc 0.97890
20:42:47.414      Training iter 50, batch loss 0.0002, batch acc 0.9968
20:42:47.673      Training iter 100, batch loss 0.0002, batch acc 0.9978
20:42:47.924      Training iter 150, batch loss 0.0002, batch acc 0.9958
20:42:48.189      Training iter 200, batch loss 0.0002, batch acc 0.9976
20:42:48.435      Training iter 250, batch loss 0.0002, batch acc 0.9966
20:42:48.692      Training iter 300, batch loss 0.0002, batch acc 0.9970
20:42:48.929      Training iter 350, batch loss 0.0002, batch acc 0.9978
20:42:49.177      Training iter 400, batch loss 0.0002, batch acc 0.9966
20:42:49.298      Training iter 450, batch loss 0.0002, batch acc 0.9966
20:42:49.421      Training iter 500, batch loss 0.0002, batch acc 0.9974
20:42:49.612      Training iter 550, batch loss 0.0002, batch acc 0.9956
20:42:49.798      Training iter 600, batch loss 0.0002, batch acc 0.9960
20:42:49.812 Testing @ 50 epoch...
20:42:49.982      Testing, total mean loss 0.00068, total acc 0.97810
20:42:50.256      Training iter 50, batch loss 0.0002, batch acc 0.9976
20:42:50.503      Training iter 100, batch loss 0.0002, batch acc 0.9980
20:42:50.731      Training iter 150, batch loss 0.0002, batch acc 0.9974
20:42:50.953      Training iter 200, batch loss 0.0002, batch acc 0.9968
20:42:51.178      Training iter 250, batch loss 0.0002, batch acc 0.9978
20:42:51.455      Training iter 300, batch loss 0.0002, batch acc 0.9978
20:42:51.715      Training iter 350, batch loss 0.0002, batch acc 0.9948
20:42:51.825      Training iter 400, batch loss 0.0002, batch acc 0.9948
20:42:51.939      Training iter 450, batch loss 0.0002, batch acc 0.9956
20:42:52.096      Training iter 500, batch loss 0.0001, batch acc 0.9984
20:42:52.352      Training iter 550, batch loss 0.0002, batch acc 0.9958
20:42:52.634      Training iter 600, batch loss 0.0002, batch acc 0.9962
20:42:52.636 Testing @ 51 epoch...
20:42:52.810      Testing, total mean loss 0.00070, total acc 0.97870
20:42:53.018      Training iter 50, batch loss 0.0002, batch acc 0.9972
20:42:53.205      Training iter 100, batch loss 0.0002, batch acc 0.9986
20:42:53.340      Training iter 150, batch loss 0.0002, batch acc 0.9964
20:42:53.452      Training iter 200, batch loss 0.0002, batch acc 0.9976
20:42:53.610      Training iter 250, batch loss 0.0002, batch acc 0.9966
20:42:53.817      Training iter 300, batch loss 0.0001, batch acc 0.9978
20:42:54.060      Training iter 350, batch loss 0.0002, batch acc 0.9960
20:42:54.298      Training iter 400, batch loss 0.0002, batch acc 0.9972
20:42:54.556      Training iter 450, batch loss 0.0002, batch acc 0.9974
20:42:54.813      Training iter 500, batch loss 0.0002, batch acc 0.9984
20:42:55.053      Training iter 550, batch loss 0.0002, batch acc 0.9966
20:42:55.302      Training iter 600, batch loss 0.0002, batch acc 0.9970
20:42:55.316 Testing @ 52 epoch...
20:42:55.510      Testing, total mean loss 0.00069, total acc 0.97740
20:42:55.766      Training iter 50, batch loss 0.0002, batch acc 0.9974
20:42:56.031      Training iter 100, batch loss 0.0002, batch acc 0.9982
20:42:56.277      Training iter 150, batch loss 0.0002, batch acc 0.9966
20:42:56.550      Training iter 200, batch loss 0.0002, batch acc 0.9980
20:42:56.822      Training iter 250, batch loss 0.0002, batch acc 0.9982
20:42:57.080      Training iter 300, batch loss 0.0002, batch acc 0.9972
20:42:57.333      Training iter 350, batch loss 0.0002, batch acc 0.9976
20:42:57.597      Training iter 400, batch loss 0.0002, batch acc 0.9976
20:42:57.876      Training iter 450, batch loss 0.0002, batch acc 0.9978
20:42:58.142      Training iter 500, batch loss 0.0002, batch acc 0.9970
20:42:58.395      Training iter 550, batch loss 0.0002, batch acc 0.9972
20:42:58.660      Training iter 600, batch loss 0.0002, batch acc 0.9958
20:42:58.662 Testing @ 53 epoch...
20:42:58.861      Testing, total mean loss 0.00068, total acc 0.97860
20:42:59.130      Training iter 50, batch loss 0.0002, batch acc 0.9980
20:42:59.393      Training iter 100, batch loss 0.0002, batch acc 0.9978
20:42:59.658      Training iter 150, batch loss 0.0002, batch acc 0.9980
20:42:59.921      Training iter 200, batch loss 0.0002, batch acc 0.9978
20:43:00.171      Training iter 250, batch loss 0.0002, batch acc 0.9972
20:43:00.423      Training iter 300, batch loss 0.0002, batch acc 0.9972
20:43:00.693      Training iter 350, batch loss 0.0001, batch acc 0.9976
20:43:00.964      Training iter 400, batch loss 0.0002, batch acc 0.9972
20:43:01.220      Training iter 450, batch loss 0.0002, batch acc 0.9968
20:43:01.477      Training iter 500, batch loss 0.0002, batch acc 0.9978
20:43:01.632      Training iter 550, batch loss 0.0001, batch acc 0.9982
20:43:01.733      Training iter 600, batch loss 0.0001, batch acc 0.9982
20:43:01.740 Testing @ 54 epoch...
20:43:01.798      Testing, total mean loss 0.00070, total acc 0.97820
20:43:02.013      Training iter 50, batch loss 0.0002, batch acc 0.9980
20:43:02.246      Training iter 100, batch loss 0.0002, batch acc 0.9988
20:43:02.455      Training iter 150, batch loss 0.0001, batch acc 0.9986
20:43:02.712      Training iter 200, batch loss 0.0001, batch acc 0.9984
20:43:02.974      Training iter 250, batch loss 0.0002, batch acc 0.9972
20:43:03.222      Training iter 300, batch loss 0.0002, batch acc 0.9964
Traceback (most recent call last):
  File "D:\involuntary\works\classes\ANN\HW1\codes\run_mlp.py", line 87, in <module>
    processor(model, loss, tr_data, te_data, tr_label, te_label, train_config)
  File "D:\involuntary\works\classes\ANN\HW1\codes\run_mlp.py", line 71, in processor
    iteration = train_net(mdl, los, config, train_data, train_label, config['batch_size'], config['disp_freq'])
  File "D:\involuntary\works\classes\ANN\HW1\codes\solve_net.py", line 33, in train_net
    model.backward(grad)
  File "D:\involuntary\works\classes\ANN\HW1\codes\network.py", line 21, in backward
    grad_input = self.layer_list[i].backward(grad_input)
  File "D:\involuntary\works\classes\ANN\HW1\codes\layers.py", line 139, in backward
    return ret.copy()
KeyboardInterrupt
